#### ch3复习
# 数学底层原理是Banach不动点定理，可以用迭代的方法求解Bellman期望方程
#
# 策略迭代 利用动态规划更新策略
# 利用Bellman期望公式，通过bootstrap方式得到状态价值V(s)
# 构造q(s,a), 用状态价值V表示动作价值q(s,a) ，确定性策略
# 更新策略policy, \pi(s) = \argmax q(s,a)

# 值迭代
# 利用Bellman最优公式， 算出最优动作价值函数q_*(s,a)
# 用最优动作价值q_*(s,a)表示最优状态价值 v_*(s,a)=\max_{a \in A} q_*(s,a)
# 再将最优状态价值V_*(s)表示最优动作价值q_*(s,a)
# 最后更新策略 policy, \pi(s)=\argmax q_*(s,a)

#### FreeModel 无模型环境
# 无模型的机器学习算法在没有数学描述的环境（为环境建立精确的数学模型及其困难）下，
# 只能依靠经验（如轨迹的样本）学习出给定策略的价值函数和最优策略
#### 回合更新价值迭代
## 回合更新策略评估：用Monte Carlo方法来估计这个期望
## 在model-based下，状态价值和动作价值可以互相表示
# 任意策略的价值函数满足Bellman期望方程，借助动力p（环境转移模型），可以用状态价值函数表示动作价值函数
# 状态价值函数->动作价值函数  q_\pi(s,a) = \sum_{s'} p(s'|s,a)*v_\pi(s')
# 借助策略\pi的表达式，可以动作价值表示状态价值
# 动作价值->状态价值 v_\pi(s) = \sum_{a} \pi(a|s)q_\pi(s,a)
## 在free-model 下，p的表达式未知，只能用动作价值表示状态价值，反之则不行。
# 由于策略改进可以仅由动作价值函数确定，因此学习问题中，动作函数往往更加重要。（\pi(s) = \argmax_{a \in A} q_\pi(s,a)）


