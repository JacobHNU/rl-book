'''
事实上，可以证明用感知机组成的网络可以表示所有原子布尔运算。

一、前向传播
1. 单层神经元的前向传播：
(1)线性映射过程
    从第 l-1 层传播到第 l 层， y^{(l-1)} -> z^{(l)}   z^{(l)}_{j}=\sum^{n_{(l-1)}}_{i=1}w^{(l)}_{ij}y^{(l-1)}_{i}+b^{(l)}_j
(2)激活函数过程
    位于第 l 层  z^{(l)} -> y^{(l)}   y^{(l)}_j = \sigma (z^{(l)}_j)   j = 1,2,...,n_l
(3)矩阵形式
    Y^(l) = \sigma (  (W^{(l)})^{\mathrm T} Y^{(l-1)} + B^{(l)}  )

2. 所有前向传播过程
X ==> Y^(0) --(z^(1))--> Y^(1) --> ...... --> Y^(l-1) --(z^(l))-->Y^(l) --> ... ==> Y^hat （Y^hat就是神经网络的预测值）

=>深度神经网络其实就是有线性函数和激活函数经过多层复合而成的一个高度非线性的映射，其非线性的性质的源自于激活函数。

二、训练模型
1. 利用 预测值Y^hat 和 有监督学习中的标签Y 构建loss function， 一般使用MSE（mean square error）来构建loss，然后就是迭代降低loss

传统优化问题与神经网络求解问题的区别：
    1. 决策变量(变化量)
    传统优化决策变量是 f(x)中的x
    神经网络中的决策变量是参数, W和B
    2. 目标函数
    传统优化问题的 f(x)中的参数是固定的，目标函数的表达式是不变的，变得是决策变量x
    神经网络的目标函数：  一般训练数据是采取mini-batch（大小通常为32），每次训练得数据不同，训练过程中参数会调整，也即目标函数会变化

=> 深度神经网络的训练模型也是一个参数优化的问题。
2. 误差反向传播
    在中间环节有 Y_{k+1} = Y_{k} - \alpha (\nabla f(x_k))    # \nabla 是梯度的符号，markdown表示方式
    链式求导法则：
     \frac{ \partial L_B } {\partial Y^{(1)}}  <-- ...<- \frac{\partial L_B}{\partial Y^{(s-1)}} <-- \frac{\partial L_B}{\partial Y(s)}

     （1） 首先在前向传播的终点Y^{(s)}，构建的loss function: L()函数,
            通过 \frac {\partial Y^{(s)}} {\partial W^{(s)}} 和 \frac {\partial Y^{(s)}} {\partial B^{(s)}}
            得到 \frac {\partial L_B} {\partial W^{(s)}} 和 \frac {\partial L_B } {\partial B^{(s)}}
     （2）然后通过中间环节的 Y_{k+1} = Y_{k} - \alpha (\nabla f(x_k)) ，
            作为中间传递的求导， \frac{ \partial Y^{(s)} } {\partial Y^{(s-1)}}
                得到 \frac{ \partial L_B } {\partial Y^{(s-1)}}
     （3）依次从 s 传递到 1，
        就得到了\frac{ \partial L_B } {\partial Y^{(1)}}， 然后再对通过 Y 对 W 和 B 求导


三、激活函数
    （1）单调可微性  激活函数一般是单调可微的，单调性保证了输出随着输入的增加而增加，可微性保证了在反向传播时包含了激活函数的式子可导
    （2）非线性     激活函数是深度神经网络的非线性特征的唯一来源
    （3）导数有界性  激活函数的导数必须有界，保证了在反向传播时不会因为激活函数的导数过大导致梯度爆炸

    1、sigmoid函数：
        适用于二分类问题，但是不是奇函数不以原点为中心，因此不能以-1或1作为标签，而是要改成0,1。
        计算涉及指数函数，计算量较大。
        容易梯度消失
        相关概念：梯度饱和或梯度弥散。
    2、Tanh函数
        以0为中心，适用于二分类
        计算量大
        容易梯度消失
    #上面两种激活函数一般用于输出层全连接层#
    #中间层神经元一般用#
    3、ReLU函数
        计算简单，常用于中间隐藏层
        输入为正时，梯度恒为1；输入为负时，梯度恒为0，出现梯度弥散。
        输出不对称
    4、ELU函数
        对ReLU函数的改进，区别是在0点处，导数存在
        导数很接近0时，还是会出现梯度弥散问题
        涉及指数计算，计算量较大
    5、Softmax函数
        处理多分类问题，一般用在网络输出层（全连接层）之后的多分类环节。
四、损失函数（损失函数是针对单个训练样本的，代价函数是对于全部训练样本的，目标函数是要优化的对象）
    预测输出，目标输出。
    1. L1范数损失函数   衡量小批量数据的预测输出和目标输出的绝对误差之和或者均值
    2. 均方误差损失函数               预测输出与目标输出的误差平方之和或者均值
    3. 负对数似然损失函数    应用于logistic回归（二分类问题）中
    4. 交叉熵损失函数      用于多分类问题
    5. KL散度损失函数      比较两个概率分布的接近程度
五、数据预处理
    归一化处理
        不同评价指标（即特征向量中的不同特征就是所述的不同评价指标）往往具有不同的量纲和量纲单位。处理后使得各指标处于同一数量级。
        归一化的目的就是使得待处理的数据被限定在一定的范围内，从而消除奇异样本数据导致的不良影响。
    标准化处理
        将训练集中某一列数值特征的值缩放成均值为0，方差为1的状态。
    选择处理方法标准：
    1、 如果数据集小而稳定，可以选择归一化
    2、如果数据集中含有噪声和异常值，选择标准化，标准化更适合嘈杂的大数据集

六、利用pytorch构建神经网络：
    1. 使用封装好的模块，按照约定的结构搭建即可。
    2. 封装好的模块都放在torch.nn库中，包括卷积层（Convolution Layers）,池化层（Pooling Layers）,
        边界填充层（Padding Layers）,激活函数（Activation Function）,线性层（Linear Layers）等
'''

import torch
import torch.nn as nn

# [线性层]
# torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)
Bath_size = 10  # batch size
linear_layer = nn.Linear(20, 30)  # 创建线性层实例 得到A.size() = [30,20]
x = torch.randn(Bath_size,20)  # 输入批量数据，单个输入维度为20,
y = linear_layer(x)            # 线性层映射，输出y   具体计算时这里需要转置 [10, 20] x [20, 30]
# print(y.size(), y.dtype)
'''
torch.Size([10, 30]) torch.float32
'''

dir(linear_layer)           # 查询linear_layer的所有属性和功能函数名    此方法无效
# print(linear_layer.weight, linear_layer.weight.size())  # 查询权重、
'''
得到 torch.Size([30, 20])
tensor([[-0.1793, -0.1767, -0.0958,  0.0812, -0.0280, -0.0209,  0.0979,  0.2202,
          0.1845, -0.2230, -0.1258, -0.2118, -0.1241, -0.0662, -0.2076, -0.0769,
          0.1995,  0.2009, -0.1609, -0.0101],
        [ 0.1287, -0.0138, -0.0899,  0.0753, -0.1121,  0.1818,  0.0582, -0.1038,
          0.1648,  0.2046, -0.1614,  0.0868, -0.0681, -0.0362, -0.1494,  0.1392,
         -0.1855, -0.0745,  0.1223,  0.1124],
        [-0.0432, -0.1195,  0.1554, -0.1862, -0.0949,  0.0334, -0.0501,  0.1637,
          0.0318, -0.0547, -0.0390, -0.1201, -0.0991,  0.1620, -0.0484,  0.0449,
         -0.0259, -0.0813,  0.1278, -0.1786],
        [-0.0048,  0.1827, -0.1311,  0.0505, -0.0888, -0.1274, -0.1805,  0.1295,
         -0.0205, -0.1019, -0.1678,  0.0830, -0.2209, -0.0335,  0.1382, -0.0365,
         -0.0835, -0.2053, -0.0545, -0.0640],
        [ 0.1072, -0.0066,  0.1263, -0.1076, -0.2067,  0.1978, -0.0589, -0.1836,
         -0.0745, -0.0134,  0.2046,  0.0999,  0.0917, -0.0233,  0.0672,  0.2067,
         -0.1839, -0.1165,  0.1947, -0.1934],
        [ 0.1657, -0.2051, -0.0987,  0.1458, -0.0655,  0.0241, -0.0615,  0.2113,
         -0.0202,  0.1281,  0.1795, -0.1444,  0.0497, -0.0851,  0.1356,  0.1314,
          0.1925, -0.1823, -0.0760,  0.1268],
        [-0.1799,  0.1970, -0.0072,  0.1477, -0.1771, -0.1291, -0.2011, -0.2009,
          0.0610,  0.0597, -0.0406, -0.2186,  0.1400, -0.0323,  0.1353,  0.1810,
         -0.1406, -0.0465,  0.0013, -0.0270],
        [ 0.1279,  0.1454,  0.0559, -0.0885, -0.1736, -0.1855,  0.1339, -0.1340,
          0.0165, -0.0359,  0.0833, -0.0639, -0.1519,  0.1055, -0.0721,  0.0123,
         -0.1963, -0.1637, -0.2095,  0.0211],
        [ 0.1893, -0.2085,  0.1506,  0.1513, -0.0413, -0.2124, -0.2069,  0.1196,
          0.1212, -0.0628, -0.2159,  0.1276,  0.1405, -0.0659, -0.0687,  0.0977,
          0.0139, -0.0719,  0.1982, -0.0631],
        [ 0.0132, -0.1951, -0.1665, -0.1831,  0.0368,  0.0873,  0.1280,  0.1774,
          0.1994, -0.1559,  0.1577,  0.1707, -0.1003,  0.1220, -0.1671, -0.1993,
          0.0443, -0.0063,  0.1029,  0.0549],
        [-0.0806, -0.0951,  0.1484,  0.0410, -0.1539, -0.1959,  0.1636, -0.0701,
         -0.0999,  0.0098, -0.1439,  0.0827, -0.1165, -0.1952,  0.0134, -0.1860,
          0.1127,  0.1892, -0.1861,  0.1235],
        [-0.1728,  0.0826, -0.2152, -0.0511,  0.1472,  0.1155,  0.1165, -0.0725,
          0.1481,  0.1235, -0.0361,  0.2193,  0.1669, -0.0236, -0.0856,  0.0732,
         -0.2062,  0.1060,  0.0834, -0.0237],
        [-0.0041, -0.0536, -0.0196, -0.0084,  0.1354,  0.0768,  0.0830,  0.0336,
         -0.1181, -0.2068, -0.0943,  0.0076,  0.0468, -0.1325,  0.1725,  0.1767,
          0.1198, -0.1639, -0.0099, -0.2039],
        [-0.0599,  0.0192, -0.0929, -0.0490,  0.0214,  0.0395, -0.1153,  0.0877,
         -0.0583, -0.1931,  0.1179,  0.1204,  0.0103, -0.0263,  0.2235,  0.0036,
          0.2062, -0.0640,  0.0847,  0.0743],
        [ 0.1868, -0.1585, -0.0036,  0.0985, -0.1254,  0.1770, -0.0395,  0.1676,
          0.2059, -0.1425, -0.2059, -0.1369,  0.0377, -0.0150,  0.2051, -0.1531,
         -0.0985, -0.0306,  0.0939,  0.2220],
        [ 0.1814, -0.2099, -0.1618,  0.0876,  0.1812, -0.1425,  0.0950,  0.1189,
          0.0703, -0.0717, -0.2169,  0.0759, -0.1350,  0.1505,  0.0289, -0.0757,
         -0.1670, -0.1195,  0.2002, -0.0519],
        [ 0.2017, -0.0160, -0.0125,  0.0439, -0.1249, -0.0299,  0.1065,  0.1903,
          0.0603, -0.0712,  0.0123,  0.0638,  0.0588,  0.0341,  0.0727, -0.2096,
         -0.0281, -0.1752,  0.1183,  0.1388],
        [ 0.2166,  0.0385, -0.1947, -0.1090, -0.0108,  0.0192, -0.0906,  0.0441,
          0.1776,  0.0893,  0.0368,  0.1982,  0.1870,  0.0243,  0.1666, -0.0957,
         -0.2095,  0.1862,  0.1879,  0.0119],
        [-0.0954, -0.2234, -0.1582,  0.0096, -0.1162,  0.1414,  0.1920,  0.0711,
          0.0526,  0.1124,  0.2138,  0.0436,  0.1044,  0.0388,  0.1141,  0.0925,
          0.0481,  0.2009,  0.1319,  0.0979],
        [ 0.1677, -0.2097, -0.2135,  0.0619, -0.2025, -0.1333, -0.1244,  0.1018,
         -0.0071,  0.1667,  0.1730, -0.1217,  0.0405, -0.1719,  0.0414,  0.1123,
          0.0162, -0.0866,  0.1019,  0.1871],
        [-0.1352,  0.0547,  0.2140, -0.1099,  0.1962, -0.0287, -0.1177, -0.1503,
         -0.1221, -0.2110, -0.1345, -0.0800, -0.1957,  0.1030,  0.1396,  0.1446,
          0.1045,  0.1612, -0.0846,  0.1297],
        [ 0.1838, -0.1191,  0.1766, -0.2107, -0.1893, -0.0383, -0.1446, -0.1421,
         -0.0612,  0.2146,  0.1878,  0.1126, -0.0417, -0.1559,  0.2029, -0.1455,
          0.2142,  0.1168,  0.1214,  0.1067],
        [ 0.1342,  0.1963,  0.1425,  0.0977, -0.1482, -0.0389, -0.0164, -0.1600,
          0.0013, -0.1280, -0.2235,  0.1981,  0.0415, -0.1333,  0.0062, -0.2033,
         -0.2062, -0.1086, -0.0198, -0.0349],
        [ 0.1992, -0.0050, -0.0371,  0.0803, -0.1053,  0.2229,  0.2192,  0.0739,
         -0.1958, -0.2220,  0.0133, -0.0233, -0.1081, -0.2209,  0.2165,  0.1759,
         -0.0826, -0.0092,  0.1936, -0.1330],
        [ 0.1185,  0.1008, -0.0610, -0.0528,  0.0272,  0.1142,  0.0021, -0.1017,
          0.0605,  0.0918,  0.1751,  0.1229,  0.1887,  0.0119,  0.0928,  0.0513,
         -0.0734, -0.1965,  0.0230, -0.0466],
        [-0.1760,  0.1203, -0.1986, -0.0018, -0.1617,  0.0724, -0.0073,  0.1916,
          0.0463,  0.0325, -0.1228,  0.2221, -0.0032, -0.1266,  0.0099,  0.1632,
         -0.1241,  0.0333, -0.1326,  0.2076],
        [-0.0032,  0.1194,  0.0059, -0.1933, -0.1266, -0.0529,  0.0114,  0.1988,
         -0.0906,  0.1129,  0.0859,  0.1483,  0.0141, -0.1045,  0.1319, -0.0426,
          0.0027,  0.1122, -0.1117, -0.1064],
        [-0.1472, -0.0505, -0.1427, -0.1500, -0.0041,  0.0615, -0.0982, -0.0862,
         -0.0945, -0.2223, -0.1311, -0.1557,  0.0421, -0.0598, -0.0014, -0.1377,
          0.1910,  0.1614, -0.1472, -0.2000],
        [ 0.1942,  0.1735,  0.1892, -0.1512,  0.0809,  0.0217, -0.1900,  0.1135,
          0.1075, -0.0974,  0.1422,  0.0560, -0.0755, -0.1157,  0.0609,  0.1381,
         -0.0392,  0.2024,  0.0422,  0.1682],
        [-0.0133,  0.1697,  0.1865,  0.0575, -0.0761, -0.0470, -0.0963, -0.0091,
          0.1485, -0.2118,  0.1154, -0.0819, -0.0660, -0.1356,  0.1633,  0.0204,
         -0.0936, -0.0798, -0.1429, -0.1714]], requires_grad=True)
'''
# print(linear_layer.bias, linear_layer.bias.size())    # 查询偏置
'''
tensor([-0.1392, -0.1017,  0.0167,  0.1606, -0.1104,  0.1360, -0.1090,  0.1581,
         0.1155,  0.1902, -0.0429,  0.1251,  0.1502,  0.1383,  0.2170,  0.0815,
         0.0895, -0.1164,  0.1199, -0.2121, -0.1550,  0.0336, -0.0232, -0.0879,
         0.1946,  0.0909,  0.1085, -0.0934, -0.0610,  0.1874],
       requires_grad=True) torch.Size([30])
'''
# 激活函数
relu = nn.ReLU()                # 创建ReLU函数实体
x = torch.randn(Bath_size, 20)  # 输入批量数据，单个输入维度为20
y = relu(x)                     # ReLU函数映射
# print(x.shape,y.shape)  # 输入输出尺寸一样
'''
torch.Size([10, 20]) torch.Size([10, 20])
'''

# MSELoss函数
# torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')
B = 10                                   # Batch size
loss = nn.MSELoss()                      # 创建MSELoss函数， reduction='mean'
loss_sum = nn.MSELoss(reduction='sum')   # 创建MSELoss函数，reduction='sum'
loss_none = nn.MSELoss(reduction='none')  # 创建MSELoss函数，reduction='none'
y_hat = torch.randn((B,5))                # 预测输出
y_tar = torch.randn((B,5))                # 目标输出
out = loss(y_hat, y_tar)                  # 损失函数值
out_sum = loss_sum(y_hat, y_tar)
out_none = loss_none(y_hat, y_tar)
print(out,out.shape)
'''
tensor(2.7946) torch.Size([])
'''
print(out_sum,out_sum.shape)
'''
tensor(139.7309) torch.Size([])
'''
print(out_none,out_none.shape)
'''
tensor([[5.4936e-01, 3.8682e-04, 8.0346e-01, 6.8689e+00, 8.8456e-02],
        [3.1645e-01, 8.4444e+00, 1.1041e+01, 7.0309e+00, 4.7544e+00],
        [9.1176e+00, 4.9526e+00, 3.2462e-01, 7.3276e-01, 8.2600e+00],
        [2.0239e-01, 2.1730e-02, 8.3386e-02, 3.6453e-02, 2.6815e+00],
        [2.1377e+00, 7.1564e+00, 4.1227e+00, 2.2439e-05, 1.0792e-01],
        [6.4928e-01, 1.9797e-01, 2.0968e+00, 3.0372e+00, 2.3613e+00],
        [1.2973e+00, 1.3531e-01, 5.8046e-01, 2.0066e+00, 1.4346e-01],
        [1.6209e+00, 6.5296e+00, 2.7608e-03, 8.0726e+00, 1.0908e+01],
        [4.0045e+00, 3.4623e+00, 1.3550e+00, 9.7634e-01, 4.4877e+00],
        [1.4054e-01, 4.2949e-01, 1.7256e-02, 2.9493e+00, 2.4328e+00]]) torch.Size([10, 5])
'''